{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 6375.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7PyKZ8bL8GI"
      },
      "source": [
        "## Goal: French to English Translation with RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8U-DmDKRiiz-"
      },
      "source": [
        "## Download dataset from Kaggle\n",
        "\n",
        "#### https://www.kaggle.com/dhruvildave/en-fr-translation-dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y305qev2hSzw"
      },
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgTP5ESthS2p"
      },
      "source": [
        "# Upload your kaggle.json file, place into the content folder\n",
        "! cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOL9MG8ehS5c"
      },
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJlSQoIYhS8T"
      },
      "source": [
        "! kaggle datasets download dhruvildave/en-fr-translation-dataset --force"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DscoJLnSHtSv"
      },
      "source": [
        "# Should have zipped file here\n",
        "! ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUTaxQLohS-d"
      },
      "source": [
        "! unzip en-fr-translation-dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0g639xDoEVrb"
      },
      "source": [
        "## Imports required"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9E38dLUYNkn"
      },
      "source": [
        "! pip install -q tensorflow_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cV6qCBEjEagx"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text\n",
        "\n",
        "sns.set_style(\"whitegrid\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbJ-leAAjsQ7"
      },
      "source": [
        "## Read dataset functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCrI5s2EkkxH"
      },
      "source": [
        "def get_total_chunks(df_chunk):\n",
        "  \"\"\" Returns count of chunk in dataset. \"\"\"\n",
        "  total_chunks = 0\n",
        "  for _, _ in enumerate(df_chunk):\n",
        "    total_chunks += 1\n",
        "  return total_chunks\n",
        "\n",
        "def get_dataset_chunk(chunk_size, num_chunks, filename, all_chunks=False):\n",
        "  \"\"\" Returns pandas DataFrame with a size of num_chunks. \"\"\"\n",
        "  df_chunk = pd.read_csv(filename, chunksize=chunk_size)\n",
        "  if (all_chunks):\n",
        "    total_chunks = get_total_chunks(df_chunk)\n",
        "    return pd.DataFrame(df_chunk.get_chunk(total_chunks))\n",
        "  else:\n",
        "    return pd.DataFrame(df_chunk.get_chunk(num_chunks))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuFsiV0wVIzQ"
      },
      "source": [
        "## Preprocess data functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAGVJnr9UzpL"
      },
      "source": [
        "def get_tensor_data(df):\n",
        "  \"\"\" Returns tensors for input and target data. \"\"\"\n",
        "  tensor_data_input = tf.convert_to_tensor(np.array(df['fr'].astype(str)))\n",
        "  tensor_data_target = tf.convert_to_tensor(np.array(df['en'].astype(str)))\n",
        "  return tensor_data_input, tensor_data_target\n",
        "\n",
        "# Source: https://www.tensorflow.org/text/tutorials/nmt_with_attention\n",
        "def tf_lower_and_split_punct(text):\n",
        "  \"\"\" Used in text processors.\n",
        "      Split accented characters. \n",
        "      Keep space, a to z, and select punctuation, add spaces around punctuation, strip whitespace.\n",
        "  \"\"\"\n",
        "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "  text = tf.strings.lower(text)\n",
        "  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
        "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
        "  text = tf.strings.strip(text)\n",
        "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "  return text\n",
        "\n",
        "def get_text_processor():\n",
        "  \"\"\" Returns TensorFlow text processors for input and target. \"\"\"\n",
        "  max_vocab_size = 50000\n",
        "\n",
        "  input_text_processor = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "      standardize=tf_lower_and_split_punct,\n",
        "      max_tokens=max_vocab_size\n",
        "  )\n",
        "\n",
        "  target_text_processor = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "      standardize=tf_lower_and_split_punct,\n",
        "      max_tokens=max_vocab_size\n",
        "  )\n",
        "\n",
        "  return input_text_processor, target_text_processor\n",
        "\n",
        "def init_text_processors(df):\n",
        "  \"\"\" Initializes text processor and returns texts processors and tensors for input and target data. \"\"\"\n",
        "  tensor_data_input, tensor_data_target = get_tensor_data(df)\n",
        "  input_text_processor, target_text_processor = get_text_processor()\n",
        "\n",
        "  input_text_processor.adapt(tensor_data_input)\n",
        "  target_text_processor.adapt(tensor_data_target)\n",
        "  \n",
        "  return input_text_processor, target_text_processor, tensor_data_input, tensor_data_target\n",
        "\n",
        "def get_dataset(filename, chunksize, num_chunks):\n",
        "  \"\"\" Calls functions that returns pandas DataFrame of original dataset. \"\"\"\n",
        "  return get_dataset_chunk(chunk_size=chunksize, num_chunks=num_chunks, filename=filename, all_chunks=False)\n",
        "\n",
        "def filter_dataset_by_string_length(df, min_length, max_length):\n",
        "  \"\"\" Returns filtered dataset based on string length. \"\"\"\n",
        "  df = df.loc[(df['en'].str.len() <= max_length) & (df['en'].str.len() >= min_length) & (df['fr'].str.len() <= max_length) & (df['fr'].str.len() >= min_length)]\n",
        "  df = shuffle(df)\n",
        "  df.reset_index(drop=True, inplace=True)\n",
        "  return df\n",
        "\n",
        "def generate_test_train(input, target, test_size=0.2):\n",
        "  \"\"\" Splits data for training and testing. \"\"\"\n",
        "  split_at = math.ceil(input.shape[0] * test_size)\n",
        "  input_test = input[:split_at]\n",
        "  target_test = target[:split_at]\n",
        "  input_train = input[split_at:]\n",
        "  target_train = target[split_at:]\n",
        "  return input_train, input_test, target_train, target_test\n",
        "\n",
        "def preprocess_data(df, row_length):\n",
        "  \"\"\" Returns one-hot encoded input and target tensors, depth, and output_size. \"\"\"\n",
        "  input_text_processor, target_text_processor, tensor_data_input, tensor_data_target = init_text_processors(df.iloc[:row_length])\n",
        "\n",
        "  vocab_size = max(input_text_processor.vocabulary_size(), target_text_processor.vocabulary_size())\n",
        "  input_vector = tf.one_hot(input_text_processor(tensor_data_input).numpy(), depth=vocab_size)\n",
        "  target_vector = tf.one_hot(target_text_processor(tensor_data_target).numpy(), depth=vocab_size)\n",
        "  output_size = target_vector.numpy().shape[1]\n",
        "\n",
        "  return input_vector, target_vector, vocab_size, output_size "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ad_jv6Xc7dj"
      },
      "source": [
        "## RNN Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MT5gH_lZ7qMS"
      },
      "source": [
        "def gradient_clip(dWaa, dWax, dWya, dBa, dBy, min_value, max_value):\n",
        "  \"\"\" Returns matrices clipped to values between min_value and max_value. \"\"\"\n",
        "  [np.clip(i, min_value, max_value, out=i) for i in [dWaa, dWax, dWya, dBa, dBy]]\n",
        "  return dWaa, dWax, dWya, dBa, dBy\n",
        "\n",
        "def init_forward_params(vocab_size, bias, output_size):\n",
        "  \"\"\" Returns initialized values for forward pass. \"\"\"\n",
        "  Waa = np.random.rand(vocab_size, vocab_size) - bias\n",
        "  Wax = np.random.rand(vocab_size, vocab_size) - bias\n",
        "  Wya = np.random.rand(output_size, vocab_size) - bias\n",
        "  Ba = np.zeros((vocab_size, 1))\n",
        "  By = np.zeros((output_size, 1))\n",
        "  return Waa, Wax, Wya, Ba, By\n",
        "\n",
        "def init_backward_params(depth, output_size, Yt, At_p, Wya):\n",
        "  \"\"\" Returns initialied values for backward pass. \"\"\"\n",
        "  dWaa = np.zeros((depth, depth))\n",
        "  dWax = np.zeros((depth, depth))\n",
        "  dBa = np.zeros((depth, 1))\n",
        "  dWya = np.matmul(Yt, At_p[len(At_p)-1].T)\n",
        "  dAt = np.matmul(Wya.T, Yt)\n",
        "  return dWaa, dWax, dBa, dWya, Yt, dAt\n",
        "\n",
        "def sigmoid(x):\n",
        "  \"\"\" Sigmoid activation function. \"\"\"\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def tanh(x):\n",
        "  \"\"\" Tanh activation function. \"\"\"\n",
        "  return np.tanh(x)\n",
        "\n",
        "def relu(x):\n",
        "  \"\"\" ReLU activation function.  \n",
        "      NumPy's maximum function returns element-wise maxima.\n",
        "  \"\"\"\n",
        "  return np.maximum(0, x)\n",
        "\n",
        "def activation(x, activation_func):\n",
        "  \"\"\" Returns proper activation function given activation_func. \"\"\"\n",
        "  if (activation_func == 'tanh'):\n",
        "    return tanh(x)\n",
        "  elif (activation_func == 'relu'):\n",
        "    return relu(x)\n",
        "  elif (activation_func == 'sigmoid'):\n",
        "    return sigmoid(x)\n",
        "\n",
        "def dActivation(dAt, At_p, activation_func):\n",
        "  \"\"\" Returns derivative of activation function given activation_func. \"\"\"\n",
        "  if (activation_func == 'tanh'):\n",
        "    return np.multiply((1 - np.power(At_p, 2)), dAt)\n",
        "  elif (activation_func == 'relu'):\n",
        "    At_p[At_p < 0] = 0.0\n",
        "    At_p[At_p > 0] = 1.0\n",
        "    return At_p\n",
        "  elif (activation_func == 'sigmoid'):\n",
        "    return sigmoid(At_p) * (1 - sigmoid(At_p))\n",
        "\n",
        "def forward(input, Waa, Wax, Wya, Ba, By, At, activation_func):\n",
        "  \"\"\" Forward pass of batched input.\n",
        "      Applies activation function given activation_func.\n",
        "      Applies Softmax to output Yt.\n",
        "      Stores each At to use in backward pass.\n",
        "\n",
        "      Based on: At = f(Waa . At-1 + Wax . Xt + ba)\n",
        "                Yt = g(Wya . At + by)\n",
        "  \"\"\"\n",
        "  At_p = []\n",
        "  At_p.append(At)\n",
        "\n",
        "  for Xt in input:\n",
        "    At = np.matmul(Waa, At) + np.matmul(Wax, Xt) + Ba\n",
        "    At = activation(At, activation_func)\n",
        "    At_p.append(At)\n",
        "\n",
        "  Yt = tf.nn.softmax(np.dot(Wya, At) + By).numpy()\n",
        "  return Yt, At_p\n",
        "\n",
        "def backward(Yt, vocab_size, output_size, At_p, Xt, Waa, Wya, activation_func):\n",
        "  \"\"\" Backward pass of batched input through time.\n",
        "      Uses derivative of activation function given activation_func.\n",
        "      Initializes values before reversed iteration.\n",
        "\n",
        "      Based on: dL / dW, at some time step.\n",
        "                dWaa += dF(At) . f(At-1)\n",
        "                dWax += dF(At) . Xt-1\n",
        "                dBa += dF(At)\n",
        "                dBy = softmax(Yt)\n",
        "  \"\"\"\n",
        "  dWaa, dWax, dBa, dWya, dBy, dAt = init_backward_params(vocab_size, output_size, Yt, At_p, Wya)\n",
        "\n",
        "  for time in reversed(range(1, len(At_p))):\n",
        "    dWaa = dWaa + np.matmul(dActivation(dAt, At_p[time], activation_func), At_p[time - 1].T)\n",
        "    dWax = dWax + np.matmul(dActivation(dAt, At_p[time], activation_func), Xt[time - 1].T)\n",
        "    dBa = dBa + dActivation(dAt, At_p[time], activation_func)\n",
        "    dAt = np.matmul(Waa.T, dActivation(dAt, At_p[time], activation_func))\n",
        "\n",
        "  return dWaa, dWax, dWya, dBa, dBy\n",
        "\n",
        "def update_weights(dWaa, dWax, dWya, dBa, dBy, Waa, Wax, Wya, Ba, By, learning_rate):\n",
        "  \"\"\" Returns updated weight values. \"\"\"\n",
        "  Waa = Waa - np.multiply(learning_rate, dWaa)\n",
        "  Wax = Wax - np.multiply(learning_rate, dWax)\n",
        "  Wya = Wya - np.multiply(learning_rate, dWya)\n",
        "  Ba = Ba - np.multiply(learning_rate, dBa)\n",
        "  By = By - np.multiply(learning_rate, dBy)\n",
        "\n",
        "  return Waa, Wax, Wya, Ba, By\n",
        "\n",
        "def rmse(target, Yt):\n",
        "  \"\"\" Returns RMSE. \"\"\"\n",
        "  return np.sqrt(np.mean(np.power((Yt - target), 2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEGhr5ahdd_t"
      },
      "source": [
        "## Plotting Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7J1fFfwJn1g"
      },
      "source": [
        "def lineplot(value_list, title, sub_title, plot_type, y_label, x_label):\n",
        "  \"\"\" Shows lineplot for given values in value_list. \"\"\"\n",
        "  x = np.arange(start=1, stop=len(value_list)+1)\n",
        "\n",
        "  if (plot_type == 'Accuracy'):\n",
        "    y = [i * 100 for i in value_list] \n",
        "  elif (plot_type == 'Loss' or plot_type == 'RMSE'):\n",
        "    y = value_list\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(12, 12))\n",
        "  ax.plot(x, y, lw=3)\n",
        "  ax.set_xlabel(x_label, fontsize=14)\n",
        "  ax.set_ylabel(y_label, fontsize=14)\n",
        "  ax.set_xlim(1, len(value_list))\n",
        "  plt.suptitle(title, y=0.92, fontsize=18)\n",
        "  plt.title(sub_title, fontsize=12)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvQXsYqYdjO2"
      },
      "source": [
        "## Create RNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpvHMBu6-mge"
      },
      "source": [
        "def generate_model(df, activation_func, learning_rate, epochs, init_matrix_bias, row_length, test_size, min_gradient_clip, max_gradient_clip):\n",
        "  \"\"\" Creates Simple RNN model. \n",
        "      Calls functions to get training and testing data.\n",
        "      Does forward and backward pass on training data.\n",
        "      Does forward pass on testing data.\n",
        "      Uses TensorFlow to get loss and accuracy values.\n",
        "      Returns lists of accuracy, RMSE, and loss values for testing and training data.\n",
        "  \"\"\"\n",
        "  df = shuffle(df)\n",
        "  df.reset_index(drop=True, inplace=True)\n",
        "  input_vector, target_vector, vocab_size, output_size = preprocess_data(df, row_length)\n",
        "  input_train, input_test, target_train, target_test = generate_test_train(input_vector.numpy(), target_vector.numpy(), test_size)\n",
        "\n",
        "  Waa, Wax, Wya, Ba, By = init_forward_params(vocab_size, init_matrix_bias, output_size)\n",
        "\n",
        "  loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "  accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "  train_loss_list = []\n",
        "  train_acc_list = []\n",
        "  train_rmse_list = []\n",
        "\n",
        "  test_loss_list = []\n",
        "  test_acc_list = []\n",
        "  test_rmse_list = []\n",
        "\n",
        "  for _ in range(epochs):\n",
        "    train_loss = 0\n",
        "    test_loss = 0\n",
        "    train_rmse = 0\n",
        "    test_rmse = 0\n",
        "\n",
        "    for train_input, train_target in zip(input_train, target_train):\n",
        "      At = np.zeros((vocab_size, vocab_size))\n",
        "      Yt, At_p = forward(train_input, Waa, Wax, Wya, Ba, By, At, activation_func)\n",
        "      train_loss += loss(train_target, Yt)\n",
        "      train_rmse += rmse(train_target, Yt)\n",
        "      accuracy.update_state(train_target[:, 0], Yt[:, 0])\n",
        "      dWaa, dWax, dWya, dBa, dBy = backward(Yt, vocab_size, output_size, At_p, train_input, Waa, Wya, activation_func)\n",
        "      dWaa, dWax, dWya, dBa, dBy = gradient_clip(dWaa, dWax, dWya, dBa, dBy, min_gradient_clip, max_gradient_clip)\n",
        "      Waa, Wax, Wya, Ba, By = update_weights(dWaa, dWax, dWya, dBa, dBy, Waa, Wax, Wya, Ba, By, learning_rate)\n",
        "\n",
        "    train_acc_list.append(accuracy.result().numpy())\n",
        "    train_loss_list.append(train_loss.numpy())\n",
        "    train_rmse_list.append(train_rmse)\n",
        "    \n",
        "    for test_input, test_target in zip(input_test, target_test):\n",
        "      At = np.zeros((vocab_size, vocab_size))\n",
        "      Yt, _ = forward(test_input, Waa, Wax, Wya, Ba, By, At, activation_func)\n",
        "      test_loss += loss(test_target, Yt)\n",
        "      test_rmse += rmse(test_target, Yt)\n",
        "      accuracy.update_state(test_target[:, 0], Yt[:, 0])\n",
        "\n",
        "    test_acc_list.append(accuracy.result().numpy())\n",
        "    test_loss_list.append(test_loss.numpy())\n",
        "    test_rmse_list.append(test_rmse)\n",
        "\n",
        "  return train_acc_list, test_acc_list, train_loss_list, test_loss_list, train_rmse_list, test_rmse_list, vocab_size, output_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_s0Rttsdo5t"
      },
      "source": [
        "## Main Driver"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEwnMucOXHrU"
      },
      "source": [
        "def init_df(filename, chunksize, num_chunks):\n",
        "  \"\"\" Returns original dataset. \"\"\"\n",
        "  return get_dataset(filename, chunksize, num_chunks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mR9rBaVfXMGj"
      },
      "source": [
        "# Get original data (number of rows = number of chunks), but row length filtering happens in preprocess_data()\n",
        "filename = 'en-fr.csv'\n",
        "chunksize = 500000\n",
        "num_chunks = 1500000\n",
        "df_ = init_df(filename, chunksize, num_chunks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wR2SmKxfXVrh"
      },
      "source": [
        "# Set min and max string length to filter dataset on\n",
        "min_length = 0\n",
        "max_length = 20\n",
        "df = filter_dataset_by_string_length(df_, min_length, max_length)\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWT78SKx4ekO"
      },
      "source": [
        "# Model Details\n",
        "activation_used = 'sigmoid' # relu, tanh, or sigmoid\n",
        "learning_rate = 0.001\n",
        "epochs = 20\n",
        "row_length = 150\n",
        "init_matrix_bias = 0.5\n",
        "test_size = 0.2\n",
        "train_size = 1 - test_size\n",
        "min_gradient_clip = 0\n",
        "max_gradient_clip = 1\n",
        "\n",
        "# Generate Model\n",
        "train_acc_list, test_acc_list, train_loss_list, test_loss_list, train_rmse_list, test_rmse_list, vocab_size, output_size = generate_model(\n",
        "    df, \n",
        "    activation_used, \n",
        "    learning_rate, \n",
        "    epochs, \n",
        "    init_matrix_bias, \n",
        "    row_length, \n",
        "    test_size, \n",
        "    min_gradient_clip, \n",
        "    max_gradient_clip)\n",
        "\n",
        "# Parameters Chosen\n",
        "print('\\nVocab Size of RNN:', vocab_size)\n",
        "print('Depth Size of RNN:', output_size)\n",
        "print('Activation Function:', activation_used.capitalize())\n",
        "print('Output Function:', 'Softmax')\n",
        "print('Number of Epochs:', epochs)\n",
        "print('Learning Rate:', learning_rate)\n",
        "print('Min/Max String Length:', f'[{min_length}, {max_length}]')\n",
        "\n",
        "# Results\n",
        "print('\\nTrain/Test Split = ', f'{int(train_size * 100)}:{int(test_size * 100)}')\n",
        "print('Size of Dataset =', row_length)\n",
        "print('Training Accuracy =', f'{round(max(train_acc_list) * 100, 2)}%')\n",
        "print('Testing Accuracy =', f'{round(max(test_acc_list) * 100, 2)}%')\n",
        "print('Training Loss =', round(np.mean(train_loss_list), 2))\n",
        "print('Testing Loss =', round(np.mean(test_loss_list), 2))\n",
        "print('Training RMSE =', round(np.sum(train_rmse_list), 2))\n",
        "print('Testing RMSE =', round(np.sum(test_rmse_list), 2))\n",
        "print()\n",
        "\n",
        "# Create plots\n",
        "# Training Accuracy\n",
        "plot_title = 'Accuracy per Epoch for Training Data'\n",
        "sub_title = f'Activation: {activation_used.capitalize()}, Epochs: {epochs}, Depth: {output_size}, Learning Rate: {learning_rate}, Instances: {row_length}, Vocab Size: {vocab_size}'\n",
        "lineplot(train_acc_list, title=plot_title, sub_title=sub_title, plot_type='Accuracy', y_label='Accuracy (%)', x_label='Epochs')\n",
        "\n",
        "# Testing Accuracy\n",
        "plot_title = 'Accuracy per Epoch for Testing Data'\n",
        "lineplot(test_acc_list, title=plot_title, sub_title=sub_title, plot_type='Accuracy', y_label='Accuracy (%)', x_label='Epochs')\n",
        "\n",
        "# Training Loss\n",
        "plot_title = 'Loss per Epoch for Training Data'\n",
        "lineplot(train_loss_list, title=plot_title, sub_title=sub_title, plot_type='Loss', y_label='Loss', x_label='Epochs')\n",
        "\n",
        "# Testing Loss\n",
        "plot_title = 'Loss per Epoch for Testing Data'\n",
        "lineplot(test_loss_list, title=plot_title, sub_title=sub_title, plot_type='Loss', y_label='Loss', x_label='Epochs')\n",
        "\n",
        "# Training RMSE\n",
        "plot_title = 'RMSE per Epoch for Training Data'\n",
        "lineplot(train_rmse_list, title=plot_title, sub_title=sub_title, plot_type='RMSE', y_label='RMSE', x_label='Epochs')\n",
        "\n",
        "# Testing RMSE\n",
        "plot_title = 'RMSE per Epoch for Testing Data'\n",
        "lineplot(test_rmse_list, title=plot_title, sub_title=sub_title, plot_type='RMSE', y_label='RMSE', x_label='Epochs')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVCk6veXeqvv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}